# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k8tQPqcjq7b0olt5F91ZD2NXolKA3ac_
"""

from pyspark.sql import SparkSession
import pyspark.sql.functions as funs

import sparknlp
sparknlp.start()

spark = SparkSession.builder.master("local[*]").getOrCreate()

# reading inputfile
import sys
input_string = sys.argv[1]
filename = "hdfs://10.0.1.111:9000/" + input_string
#print(filename)
data = spark.read.option("header", 'true')\
    .option("delimiter","\t")\
    .csv(filename).limit(1000)

# dropping columns to free up space
data = data.drop('marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_date')

# changing dataset column review_body to have all lowercase letters
data = data.withColumn('review_body', funs.lower(funs.col('review_body')))

data.coalesce(1).write.csv('Big_Data/filtered_baby_data', header="true",mode="overwrite", sep='\t')

data.unpersist()

filtered_data = spark.read.option("header", 'true')\
  .option("delimiter", "\t")\
  .csv("Big_Data/filtered_baby_data")

#pipeline for training data
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import HashingTF, IDF
stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', \
                                   'ours', 'ourselves', 'you', "you're", "you've", \
                                   "you'll", "you'd", 'your', 'yours','yourself', \
                                   'yourselves', 'he', 'him', 'his', 'himself', \
                                   'she',\
        "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itslef',\
        'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\
        'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is',\
        'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\
        'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\
        'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\
        'with', 'about', 'against', 'between', 'into', 'through', 'during',\
        'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',\
        'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\
        'once', 'here', 'there', 'when', 'where', 'why', 'how','all', 'any',\
        'both', 'each', 'few', 'more', 'most',' other', 'some', 'too','very',\
        's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've",\
        'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't",\
        'could', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn',\
        "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma',\
        'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan',\
        "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't",\
        'won', "won't", 'wouldn', "wouldn't", '/><br', '', 'than', 'would', '']

documentAssembler = DocumentAssembler() \
    .setInputCol("review_body") \
    .setOutputCol("document")

sentenceDetector = SentenceDetector() \
    .setInputCols(["document"]) \
    .setOutputCol("sentence")

tokenizer = Tokenizer() \
    .setInputCols(["sentence"]) \
    .setOutputCol("token")

'''not working. will try to add later
lemmatizer = LemmatizerModel.pretrained() \
    .setInputCols(["token"]) \
    .setOutputCol("lemma")
'''

top_words = StopWordsCleaner() \
    .setInputCols(["token"]) \
    .setOutputCol("cleanlemma")\
    .setStopWords(stopwords)

pipeline = Pipeline() \
    .setStages([
      documentAssembler,
      sentenceDetector,
      tokenizer,
      #lemmatizer,
      top_words,
    ])

pipeline_model = pipeline.fit(data)
result = pipeline_model.transform(data)
#result.selectExpr("cleanlemma.result").show(truncate=False)

cleanlemma = result.select("cleanlemma.result")

wordCount = cleanlemma.withColumn('word', funs.explode(funs.col('result')))\
  .groupBy('word')\
  .count()\
  .count()

from pyspark.ml.feature import HashingTF, IDF
hashingTF = HashingTF(inputCol="result", outputCol="features")
hashingTF.setNumFeatures(wordCount)
tfreq = hashingTF.transform(cleanlemma)
tfreq.cache()

idf = IDF(minDocFreq=int(wordCount/100))
idf.setInputCol("features")
idf.setOutputCol("idf")
model = idf.fit(tfreq)
#model.save('baby_idf')
idf_data = model.transform(tfreq)
from pyspark.ml.linalg import Vectors
from pyspark.ml.functions import vector_to_array
from pyspark.mllib.linalg import Vectors as OldVectors
idf_vector = idf_data.select(vector_to_array('idf', 'float32').alias('vectors'))

import tensorflow as tf
from keras.layers import Dense

nn_model = tf.keras.Sequential()

nn_model.add(Dense(units=500, activation='relu', input_dim=wordCount))
nn_model.add(Dense(units=1, activation='sigmoid'))
nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

nn_model.summary()

# prepares data 
idf_vector = idf_vector.toPandas()

result = result.withColumn('star_rating', funs.when(funs.col('star_rating') == '1', 0)\
                           #.when(funs.col('star_rating') == '2', 2)\
                           #.when(funs.col('star_rating') == '3', 3)\
                           #.when(funs.col('star_rating') == '4', 4)\
                           .otherwise(1))

y = result.select('star_rating').toPandas()

import numpy as np
import pandas as pd

idf_vector = pd.DataFrame(idf_vector['vectors'].tolist())

X = idf_vector.iloc[:800]
X_test = idf_vector.iloc[800:]
y_train = y['star_rating'].iloc[:800]
y_test = y['star_rating'].iloc[800:]

nn_model.fit(X, y_train, validation_data= (X_test, y_test), batch_size = 128, epochs = 20)

scores = nn_model.evaluate(X_test, y_test)

#result = result.withColumnRenamed('star_rating', 'label')

## build second model using more variables
# reading inputfile
data = spark.read.option("header", 'true')\
    .option("delimiter","\t")\
    .csv("hdfs://10.0.1.111:9000/amazon_reviews_grocery_100k.tsv").limit(1000)

# drop columns that obviously have nothing to do with predictions
data = data.drop('product_title', 'product_category', 'marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'review_date', 'review_headline', 'vine')

result2 = pipeline_model.transform(data)

# convets verified purchase to binary (from y and n to 0 and 1)
result2 = result2.withColumn('verified_purchase', funs.when(funs.col('verified_purchase') == 'Y', 1)\
                             .otherwise(0))

# converts star_rating to binary. If star rating is 1, value is changed to 0. Otherwise, the value is changed to 1
result2 = result2.withColumn('star_rating', funs.when(funs.col('star_rating') == 1, 0)\
                             .otherwise(1))

# converts helpful_votes to int
result2 = result2.withColumn('helpful_votes', result2.helpful_votes.cast('int'))

# converts total votes to int
result2 = result2.withColumn('total_votes', result2.total_votes.cast('int'))

hash2 = hashingTF.transform(result2.select('cleanlemma.result'))

idf2 = model.transform(hash2.select('features'))

result2 = result2.drop('review_body', 'document','sentence', 'token','lemma','cleanlemma')

result2 = result2.select('star_rating', 'helpful_votes', 'total_votes', 'verified_purchase').toPandas()

idf2 = idf2.select(vector_to_array('idf', 'float32').alias('vectors'))

idf2 = idf2.select('vectors').toPandas()

idf2 = pd.DataFrame(idf2['vectors'].tolist())

y2 = result2['star_rating']

result2.drop('star_rating', axis=1, inplace=True)

idf2['helpful_votes'] = result2['helpful_votes']
idf2['total_votes'] = result2['total_votes']
idf2['verified_purchase'] = result2['verified_purchase']

numcols = idf2.shape[1]

X2 = idf2.iloc[:800]
X_test2 = idf2.iloc[800:]
y_train2 = y2.iloc[:800]
y_test2 = y2.iloc[800:]

nn_model2 = tf.keras.Sequential()

nn_model2.add(Dense(units=500, activation='relu', input_dim=numcols))
nn_model2.add(Dense(units=1, activation='sigmoid'))
nn_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

nn_model2.summary()

nn_model2.fit(X2, y_train2, validation_data= (X_test2, y_test2), batch_size = 128, epochs = 20)

scores2 = nn_model2.evaluate(X_test2, y_test2)

print("Model 1 accuracy:", scores)
print("Model 2 accuracy:", scores2)

from sklearn.svm import SVC
svclassifier = SVC(kernel='rbf')
svclassifier.fit(X, y_train)

pred = svclassifier.predict(X_test)

pred

correct = 0
for i in range(len(pred)):
  if (pred[i] == y_test.iloc[i]):
    correct += 1
accuracy = (correct / len(pred)) * 100

svclassifier2 = SVC(kernel='rbf')
svclassifier2.fit(X2, y_train2)

pred2 = svclassifier2.predict(X_test2)

correct2 = 0
for i in range(len(pred2)):
  if (pred2[i] == y_test2.iloc[i]):
    correct2 += 1
accuracy2 = (correct2 / len(pred2)) * 100

print("Accuracy for first SVM model:", accuracy)
print("Accuracy for second SVM model:", accuracy2)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)
confusion_matrix(y_test2, pred2)
